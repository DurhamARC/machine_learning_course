{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real-world PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Terminology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor\n",
    "![](https://hadrienj.github.io/assets/images/2.1/scalar-vector-matrix-tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning Basic Terminology\n",
    " - Dataset\n",
    " - Loss: How far are we from goal\n",
    " - Optimizer: Improves model given loss (Using backpropogation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "<img style=\"text-align: center;\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" alt=\"Convolution\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transpose Convolution (Or \"Up-convolution\")\n",
    "<img style=\"text-align: center;\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif\" alt=\"Convolution\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img style=\"text-align: center;\" src=\"https://miro.medium.com/max/256/0*qT8D07uyUdf3SDac.gif\" alt=\"Self-driving car\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model: The learned program\n",
    "\n",
    "<img style=\"text-align: center;\" src=\"https://www.researchgate.net/profile/Brendan_Colvert/publication/319622643/figure/fig1/AS:537248807960576@1505101505202/Architecture-of-the-neural-network-consisting-of-four-successive-layers-a-linear-layer.png\" alt=\"Neural Net\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img style=\"text-align: center;\" src=\"https://miro.medium.com/max/1400/0*hgbfL5uJPVvfcAMf.png\" alt=\"Self-driving car\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Measures\n",
    " - Mean squared error (MSE)\n",
    " - Sørensen–Dice coefficient\n",
    " - Precision and Recall (PR curve)\n",
    " - Receiver operating characteristic curve (ROC curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Deep Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST\n",
    "<img style=\"text-align: center;\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" alt=\"MNIST\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 1\n",
    "GAMMA = 0.7\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images is tensor of shape: torch.Size([64, 1, 28, 28]), labels is tensor of shape: torch.Size([64])\n",
      "Images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "9,3,\n",
      "0,1,\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "test_iter = iter(test_loader)\n",
    "images, labels = train_iter.next()\n",
    "print(f'Images is tensor of shape: {images.shape}, labels is tensor of shape: {labels.shape}')\n",
    "figure = plt.figure(figsize=(1, 1))\n",
    "num_of_images = 4\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(2, 2, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index-1].numpy().squeeze(), cmap='gray_r')\n",
    "print(\"Images:\")\n",
    "plt.show()\n",
    "print(\"Labels:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(labels[i*2+j].item(), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299167\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.257756\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.168580\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.220267\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.107313\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.246762\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.123369\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.136140\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.192894\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.134642\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "ITER_PER_EPOCH = 1000\n",
    "TEST_ITER_PER_EPOCH = len(test_loader)\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_iter = itertools.cycle(iter(train_loader))\n",
    "    test_iter =  iter(test_loader)\n",
    "\n",
    "    for batch_idx in range(ITER_PER_EPOCH):\n",
    "        (data, target) = next(train_iter)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 841.1299, Accuracy: 9759/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "images = None\n",
    "preds = None\n",
    "with torch.no_grad():\n",
    "    for batch_idx in range(TEST_ITER_PER_EPOCH):\n",
    "        (data, target) = next(test_iter)\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += data.shape[0]\n",
    "        images = data.cpu()\n",
    "        preds = pred.cpu().numpy()\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images is tensor of shape: torch.Size([1000, 1, 28, 28]), preds is tensor of shape: (1000, 1)\n",
      "Images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds:\n",
      "0,1,\n",
      "4,3,\n"
     ]
    }
   ],
   "source": [
    "print(f'Images is tensor of shape: {images.shape}, preds is tensor of shape: {preds.shape}')\n",
    "figure = plt.figure(figsize=(1, 1))\n",
    "num_of_images = 4\n",
    "for index in range(num_of_images):\n",
    "    plt.subplot(2, 2, index+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n",
    "print(\"Images:\")\n",
    "plt.show()\n",
    "print(\"Preds:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(preds[i*2+j].item(), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    " - Deep learning Book: https://www.deeplearningbook.org/\n",
    " - PyTorch: https://pytorch.org/tutorials/\n",
    " - Understanding convolutions: https://github.com/vdumoulin/conv_arithmetic"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
